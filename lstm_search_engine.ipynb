{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec \n",
    "import os\n",
    "import csv\n",
    "\n",
    "def load_word2vec_model(model_file,corpus_file,vector_size):\n",
    "    if os.path.exists(model_file):\n",
    "        model= word2vec.Word2Vec.load(model_file)\n",
    "    else:\n",
    "        data =word2vec.Text8Corpus(corpus_file)\n",
    "        model=word2vec.Word2Vec(data,size=vector_size)\n",
    "        model.save(model_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "import re\n",
    "\n",
    "class SimpleStemmer:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer=stem.WordNetLemmatizer()\n",
    "        \n",
    "    def stem(self,sentence):\n",
    "        lower=sentence.lower()\n",
    "\n",
    "        #tokenize here(removing \"\\n\")\n",
    "        tokens=filter(lambda w: len(w) > 0, re.split(r'\\s|\\n',lower))\n",
    "\n",
    "        stemed=\"\"\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                stemed+=self.lemmatizer.lemmatize(token)+\" \"\n",
    "            except LookupError:\n",
    "                stemed+=token+\" \"\n",
    "            \n",
    "        return stemed[:-1]\n",
    "\n",
    "#test\n",
    "stemmer=SimpleStemmer()\n",
    "print stemmer.stem(\"Watson is cognitive\\ncomputing systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "class LstmSearchModel:\n",
    "    def __init__(self,model_file,initialize):\n",
    "        if initialize:\n",
    "            self.create_lstm_model()\n",
    "        else:\n",
    "            self.load_lstm_model(model_file)\n",
    "    \n",
    "    def load_lstm_model(model_file):\n",
    "    \n",
    "    def create_lstm_model():\n",
    "        self.lstm=rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        initial_state = state = tf.zeros([batch_size, lstm.state_size])\n",
    "        \n",
    "    \n",
    "    def get_minibatch():\n",
    "        \n",
    "    \n",
    "    def set_training_data(querys, documents,vector_cache):\n",
    "        self.w2v_tank={}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(epock):\n",
    "        for i in range(epock):\n",
    "            x,y=get_minibatch()\n",
    "            #y_=lstm(x)\n",
    "            \n",
    "            #feedback\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_query_vector():\n",
    "    \n",
    "    def get_doc_vector():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class KcDataHandler:\n",
    "    def __init__(self,file_name,stemmer,w2v_model):\n",
    "        self.file_name=file_name\n",
    "        self.stemmer=stemmer\n",
    "        self.w2v_model=w2v_model\n",
    "        \n",
    "    #vector_cache: cacheing word embedding vector as tensorflow static vector.\n",
    "    #querys and documents reference vector_cache to build their vector sequence\n",
    "    def make_training_data(self):\n",
    "        vector_cache={}\n",
    "        querys=[]\n",
    "        documents=[]\n",
    "        with open(self.file_name,\"r\") as f:\n",
    "            reader=csv.reader(f)\n",
    "            \n",
    "            #skip header\n",
    "            next(reader)\n",
    "            \n",
    "            for row in reader:\n",
    "                #row[1]:page_title row[2]:section_title\n",
    "                querys.append(self.sentence_to_seqw2v(row[1]+row[2],vector_cache))\n",
    "                #row[3] content\n",
    "                documents.append(self.sentence_to_seqw2v(row[3],vector_cache)) \n",
    "            \n",
    "        return querys,documents,vector_cache\n",
    "    \n",
    "    \n",
    "    def sentence_to_seqw2v(self,sentence,vector_cache):\n",
    "        tokens=self.stemmer.stem(sentence)\n",
    "        \n",
    "        vector_seq=[]\n",
    "        for token in tokens:\n",
    "            if not token in vector_cache.keys():\n",
    "                try:\n",
    "                    token_vec=self.w2v_model[token]\n",
    "                    token_tf_tensor=tf.convert_to_tensor([value for value in token_vec])\n",
    "                    vector_seq.append(token_tf_tensor)\n",
    "                    vector_cache[token]=token_tf_tensor\n",
    "                except KeyError:\n",
    "                    #if the token didn't registered, ignore\n",
    "                    continue\n",
    "            else:\n",
    "                vector_seq.append(vector_cache[token])\n",
    "        \n",
    "        return vector_seq\n",
    "#test\n",
    "w2v_model=load_word2vec_model(\"w2v_wiki.mod\",\"\",\"\")\n",
    "data_handler=KcDataHandler(\"kc_data.csv\",SimpleStemmer(),w2v_model)\n",
    "querys,documents,vector_cache=data_handler.make_training_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VectorSearchEngine:\n",
    "    def __init__():\n",
    "    def add_document(doc_label,doc_vector)\n",
    "    def search(query_vector)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not in use\n",
    "\n",
    "import csv\n",
    "\n",
    "#creating word2vec data from file\n",
    "data=\"kc_data.csv\"\n",
    "with open(data,\"r\") as f_data,open(\"word2vec_data.txt\",\"r\":\n",
    "    reader=csv.reader(f_data)\n",
    "    \n",
    "    #skip header\n",
    "    next(reader)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
